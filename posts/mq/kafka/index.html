<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Kafka 设计与理解 | 徐旭 的博客</title><meta name=keywords content="Kafka"><meta name=description content="整体架构  图片来自 https://zhuanlan.zhihu.com/p/103249714
 Producer 负责发布消息到Kafka broker。Producer发送消息到broker时，会根据分区策略选择将其存储到哪一个Partition。
常规的有轮询，随机等策略，主要是为了将消息均衡的发送到各个 partition，提高并行度，从而提高吞吐。常用的还有按 key 哈希，主要是为了实现业务 partition 有序的需求。
Consumer 消息消费者，从Kafka broker读取消息的客户端。
Consumer Group Consumer Group是Kafka提供的可扩展且具有容错性的消费者机制，每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。
 Consumer Group下可以有一个或多个Consumer实例。这里的实例可以是一个单独的进程，也可以是同一进程下的线程。 Group ID是一个字符串，在一个Kafka集群中，它标识唯一的一个Consumer Group。 Consumer Group下所有实例订阅的主题的单个分区，只能分配给组内的某个Consumer实例消费。  进度提交 消费者在消费的过程中需要记录自己消费了多少数据，即消费位置信息。在Kafka中，这个位置信息有个专门的术语：位移（Offset）。
Rebalance 何时触发 rebalance
 组成员数量发生变化，有新成员加入，或者有成员实例崩溃退出。 订阅主题数量发生变化：Consumer Group可以使用正则表达式的方式订阅主题，比如果consumer.subscribe(Pattern.compile(“t.*c”))就表明该Group订阅所有以字母t开头、字母c结尾的主题。在Consumer Group的运行过程中，你新创建了一个满足这样条件的主题，那么该Group就会发生Rebalance 订阅主题的分区数发生变化，如主题扩容。  Rebalance本质上是一种协议，规定了一个Consumer Group下的所有Consumer如何达成一致，来分配订阅Topic的每个分区Topic的每个分区。
比如某个Group下有20个Consumer实例，它订阅了一个具有100个分区的Topic。正常情况下，Kafka平均会为每个Consumer分配5个分区。这个分配的过程就叫Rebalance。
Rebalance 的流程 在消费者端，重平衡分为两个步骤：分别是 加入组 和 等待领导消费者（Leader Consumer）分配方案。这两个步骤分别对应两类特定的请求：JoinGroup请求和SyncGroup请求。
当组内成员加入组时，它会向 协调者 发送JoinGroup请求（后面会介绍协调者）。在该请求中，每个成员都要将自己订阅的主题上报，这样协调者就能收集到所有成员的订阅信息。一旦收集了全部成员的JoinGroup请求后，协调者会从这些成员中选择一个担任这个消费者组的领导者。
通常情况下，第一个发送JoinGroup请求的成员自动成为领导者。你一定要注意区分这里的领导者和之前我们介绍的领导者副本，它们不是一个概念。这里的领导者是具体的消费者实例，它既不是副本，也不是协调者。领导者消费者的任务是收集所有成员的订阅信息，然后根据这些信息，制定具体的分区消费分配方案。
选出领导者之后，协调者会把消费者组订阅信息封装进JoinGroup请求的响应体中，然后发给领导者，由领导者统一做出分配方案后，进入到下一步：发送SyncGroup请求。
在这一步中，领导者向协调者发送SyncGroup请求，将刚刚做出的分配方案发给协调者。值得注意的是，其他成员也会向协调者发送SyncGroup请求，只不过请求体中并没有实际的内容。这一步的主要目的是让协调者接收分配方案，然后统一以SyncGroup响应的方式分发给所有成员，这样组内所有成员就都知道自己该消费哪些分区了。
JoinGroup请求的主要作用是将组成员订阅信息发送给领导者消费者，待领导者制定好分配方案后，重平衡流程进入到SyncGroup请求阶段。
SyncGroup请求的主要目的，就是让协调者把领导者制定的分配方案下发给各个组内成员。当所有成员都成功接收到分配方案后，消费者组进入到Stable状态，即开始正常的消费工作。
正常情况下，每个组内成员都会定期汇报位移给协调者。当重平衡开启时，协调者会给予成员一段缓冲时间，要求每个成员必须在这段时间内快速地上报自己的位移信息，然后再开启正常的JoinGroup/SyncGroup请求发送。
Broker Kafka集群包含一个或多个服务器，这种服务器被称为broker。一个 broker 可以容纳多个 topic。brocker 是 kafka 中的核心组件，负责消息的存储，分区，路由信息的管理等。"><meta name=author content="徐旭"><link rel=canonical href=https://mogutou.xyz/posts/mq/kafka/><link crossorigin=anonymous href=/assets/css/stylesheet.min.04512c372388e08b5118f5b117b2d3efef4ddae52017e16085c8d8d4e361c43d.css integrity="sha256-BFEsNyOI4ItRGPWxF7LT7+9N2uUgF+FghcjY1ONhxD0=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://mogutou.xyz/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mogutou.xyz/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mogutou.xyz/favicon-32x32.png><link rel=apple-touch-icon href=https://mogutou.xyz/apple-touch-icon.png><link rel=mask-icon href=https://mogutou.xyz/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.81.0"><script async src="https://www.googletagmanager.com/gtag/js?id=G-CS73EMP38T"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-CS73EMP38T')</script><script data-ad-client=ca-pub-9617847803031506 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script><meta property="og:title" content="Kafka 设计与理解"><meta property="og:description" content="整体架构  图片来自 https://zhuanlan.zhihu.com/p/103249714
 Producer 负责发布消息到Kafka broker。Producer发送消息到broker时，会根据分区策略选择将其存储到哪一个Partition。
常规的有轮询，随机等策略，主要是为了将消息均衡的发送到各个 partition，提高并行度，从而提高吞吐。常用的还有按 key 哈希，主要是为了实现业务 partition 有序的需求。
Consumer 消息消费者，从Kafka broker读取消息的客户端。
Consumer Group Consumer Group是Kafka提供的可扩展且具有容错性的消费者机制，每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。
 Consumer Group下可以有一个或多个Consumer实例。这里的实例可以是一个单独的进程，也可以是同一进程下的线程。 Group ID是一个字符串，在一个Kafka集群中，它标识唯一的一个Consumer Group。 Consumer Group下所有实例订阅的主题的单个分区，只能分配给组内的某个Consumer实例消费。  进度提交 消费者在消费的过程中需要记录自己消费了多少数据，即消费位置信息。在Kafka中，这个位置信息有个专门的术语：位移（Offset）。
Rebalance 何时触发 rebalance
 组成员数量发生变化，有新成员加入，或者有成员实例崩溃退出。 订阅主题数量发生变化：Consumer Group可以使用正则表达式的方式订阅主题，比如果consumer.subscribe(Pattern.compile(“t.*c”))就表明该Group订阅所有以字母t开头、字母c结尾的主题。在Consumer Group的运行过程中，你新创建了一个满足这样条件的主题，那么该Group就会发生Rebalance 订阅主题的分区数发生变化，如主题扩容。  Rebalance本质上是一种协议，规定了一个Consumer Group下的所有Consumer如何达成一致，来分配订阅Topic的每个分区Topic的每个分区。
比如某个Group下有20个Consumer实例，它订阅了一个具有100个分区的Topic。正常情况下，Kafka平均会为每个Consumer分配5个分区。这个分配的过程就叫Rebalance。
Rebalance 的流程 在消费者端，重平衡分为两个步骤：分别是 加入组 和 等待领导消费者（Leader Consumer）分配方案。这两个步骤分别对应两类特定的请求：JoinGroup请求和SyncGroup请求。
当组内成员加入组时，它会向 协调者 发送JoinGroup请求（后面会介绍协调者）。在该请求中，每个成员都要将自己订阅的主题上报，这样协调者就能收集到所有成员的订阅信息。一旦收集了全部成员的JoinGroup请求后，协调者会从这些成员中选择一个担任这个消费者组的领导者。
通常情况下，第一个发送JoinGroup请求的成员自动成为领导者。你一定要注意区分这里的领导者和之前我们介绍的领导者副本，它们不是一个概念。这里的领导者是具体的消费者实例，它既不是副本，也不是协调者。领导者消费者的任务是收集所有成员的订阅信息，然后根据这些信息，制定具体的分区消费分配方案。
选出领导者之后，协调者会把消费者组订阅信息封装进JoinGroup请求的响应体中，然后发给领导者，由领导者统一做出分配方案后，进入到下一步：发送SyncGroup请求。
在这一步中，领导者向协调者发送SyncGroup请求，将刚刚做出的分配方案发给协调者。值得注意的是，其他成员也会向协调者发送SyncGroup请求，只不过请求体中并没有实际的内容。这一步的主要目的是让协调者接收分配方案，然后统一以SyncGroup响应的方式分发给所有成员，这样组内所有成员就都知道自己该消费哪些分区了。
JoinGroup请求的主要作用是将组成员订阅信息发送给领导者消费者，待领导者制定好分配方案后，重平衡流程进入到SyncGroup请求阶段。
SyncGroup请求的主要目的，就是让协调者把领导者制定的分配方案下发给各个组内成员。当所有成员都成功接收到分配方案后，消费者组进入到Stable状态，即开始正常的消费工作。
正常情况下，每个组内成员都会定期汇报位移给协调者。当重平衡开启时，协调者会给予成员一段缓冲时间，要求每个成员必须在这段时间内快速地上报自己的位移信息，然后再开启正常的JoinGroup/SyncGroup请求发送。
Broker Kafka集群包含一个或多个服务器，这种服务器被称为broker。一个 broker 可以容纳多个 topic。brocker 是 kafka 中的核心组件，负责消息的存储，分区，路由信息的管理等。"><meta property="og:type" content="article"><meta property="og:url" content="https://mogutou.xyz/posts/mq/kafka/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-05-02T11:22:38+08:00"><meta property="article:modified_time" content="2021-05-02T11:22:38+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Kafka 设计与理解"><meta name=twitter:description content="整体架构  图片来自 https://zhuanlan.zhihu.com/p/103249714
 Producer 负责发布消息到Kafka broker。Producer发送消息到broker时，会根据分区策略选择将其存储到哪一个Partition。
常规的有轮询，随机等策略，主要是为了将消息均衡的发送到各个 partition，提高并行度，从而提高吞吐。常用的还有按 key 哈希，主要是为了实现业务 partition 有序的需求。
Consumer 消息消费者，从Kafka broker读取消息的客户端。
Consumer Group Consumer Group是Kafka提供的可扩展且具有容错性的消费者机制，每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。
 Consumer Group下可以有一个或多个Consumer实例。这里的实例可以是一个单独的进程，也可以是同一进程下的线程。 Group ID是一个字符串，在一个Kafka集群中，它标识唯一的一个Consumer Group。 Consumer Group下所有实例订阅的主题的单个分区，只能分配给组内的某个Consumer实例消费。  进度提交 消费者在消费的过程中需要记录自己消费了多少数据，即消费位置信息。在Kafka中，这个位置信息有个专门的术语：位移（Offset）。
Rebalance 何时触发 rebalance
 组成员数量发生变化，有新成员加入，或者有成员实例崩溃退出。 订阅主题数量发生变化：Consumer Group可以使用正则表达式的方式订阅主题，比如果consumer.subscribe(Pattern.compile(“t.*c”))就表明该Group订阅所有以字母t开头、字母c结尾的主题。在Consumer Group的运行过程中，你新创建了一个满足这样条件的主题，那么该Group就会发生Rebalance 订阅主题的分区数发生变化，如主题扩容。  Rebalance本质上是一种协议，规定了一个Consumer Group下的所有Consumer如何达成一致，来分配订阅Topic的每个分区Topic的每个分区。
比如某个Group下有20个Consumer实例，它订阅了一个具有100个分区的Topic。正常情况下，Kafka平均会为每个Consumer分配5个分区。这个分配的过程就叫Rebalance。
Rebalance 的流程 在消费者端，重平衡分为两个步骤：分别是 加入组 和 等待领导消费者（Leader Consumer）分配方案。这两个步骤分别对应两类特定的请求：JoinGroup请求和SyncGroup请求。
当组内成员加入组时，它会向 协调者 发送JoinGroup请求（后面会介绍协调者）。在该请求中，每个成员都要将自己订阅的主题上报，这样协调者就能收集到所有成员的订阅信息。一旦收集了全部成员的JoinGroup请求后，协调者会从这些成员中选择一个担任这个消费者组的领导者。
通常情况下，第一个发送JoinGroup请求的成员自动成为领导者。你一定要注意区分这里的领导者和之前我们介绍的领导者副本，它们不是一个概念。这里的领导者是具体的消费者实例，它既不是副本，也不是协调者。领导者消费者的任务是收集所有成员的订阅信息，然后根据这些信息，制定具体的分区消费分配方案。
选出领导者之后，协调者会把消费者组订阅信息封装进JoinGroup请求的响应体中，然后发给领导者，由领导者统一做出分配方案后，进入到下一步：发送SyncGroup请求。
在这一步中，领导者向协调者发送SyncGroup请求，将刚刚做出的分配方案发给协调者。值得注意的是，其他成员也会向协调者发送SyncGroup请求，只不过请求体中并没有实际的内容。这一步的主要目的是让协调者接收分配方案，然后统一以SyncGroup响应的方式分发给所有成员，这样组内所有成员就都知道自己该消费哪些分区了。
JoinGroup请求的主要作用是将组成员订阅信息发送给领导者消费者，待领导者制定好分配方案后，重平衡流程进入到SyncGroup请求阶段。
SyncGroup请求的主要目的，就是让协调者把领导者制定的分配方案下发给各个组内成员。当所有成员都成功接收到分配方案后，消费者组进入到Stable状态，即开始正常的消费工作。
正常情况下，每个组内成员都会定期汇报位移给协调者。当重平衡开启时，协调者会给予成员一段缓冲时间，要求每个成员必须在这段时间内快速地上报自己的位移信息，然后再开启正常的JoinGroup/SyncGroup请求发送。
Broker Kafka集群包含一个或多个服务器，这种服务器被称为broker。一个 broker 可以容纳多个 topic。brocker 是 kafka 中的核心组件，负责消息的存储，分区，路由信息的管理等。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://mogutou.xyz/posts/"},{"@type":"ListItem","position":2,"name":"Kafka 设计与理解","item":"https://mogutou.xyz/posts/mq/kafka/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kafka 设计与理解","name":"Kafka 设计与理解","description":"整体架构  图片来自 https://zhuanlan.zhihu.com/p/103249714\n Producer 负责发布消息到Kafka broker。Producer发送消息到broker时，会根据分区策略选择将其存储到哪一个Partition。\n常规的有轮询，随机等策略，主要是为了将消息均衡的发送到各个 partition，提高并行度，从而提高吞吐。常用的还有按 key 哈希，主要是为了实现业务 partition 有序的需求。\nConsumer 消息消费者，从Kafka broker读取消息的客户端。\nConsumer Group Consumer Group是Kafka提供的可扩展且具有容错性的消费者机制，每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。\n Consumer Group下可以有一个或多个Consumer实例。这里的实例可以是一个单独的进程，也可以是同一进程下的线程。 Group ID是一个字符串，在一个Kafka集群中，它标识唯一的一个Consumer Group。 Consumer Group下所有实例订阅的主题的单个分区，只能分配给组内的某个Consumer实例消费。  进度提交 消费者在消费的过程中需要记录自己消费了多少数据，即消费位置信息。在Kafka中，这个位置信息有个专门的术语：位移（Offset）。\nRebalance 何时触发 rebalance\n 组成员数量发生变化，有新成员加入，或者有成员实例崩溃退出。 订阅主题数量发生变化：Consumer Group可以使用正则表达式的方式订阅主题，比如果consumer.subscribe(Pattern.compile(“t.*c”))就表明该Group订阅所有以字母t开头、字母c结尾的主题。在Consumer Group的运行过程中，你新创建了一个满足这样条件的主题，那么该Group就会发生Rebalance 订阅主题的分区数发生变化，如主题扩容。  Rebalance本质上是一种协议，规定了一个Consumer Group下的所有Consumer如何达成一致，来分配订阅Topic的每个分区Topic的每个分区。\n比如某个Group下有20个Consumer实例，它订阅了一个具有100个分区的Topic。正常情况下，Kafka平均会为每个Consumer分配5个分区。这个分配的过程就叫Rebalance。\nRebalance 的流程 在消费者端，重平衡分为两个步骤：分别是 加入组 和 等待领导消费者（Leader Consumer）分配方案。这两个步骤分别对应两类特定的请求：JoinGroup请求和SyncGroup请求。\n当组内成员加入组时，它会向 协调者 发送JoinGroup请求（后面会介绍协调者）。在该请求中，每个成员都要将自己订阅的主题上报，这样协调者就能收集到所有成员的订阅信息。一旦收集了全部成员的JoinGroup请求后，协调者会从这些成员中选择一个担任这个消费者组的领导者。\n通常情况下，第一个发送JoinGroup请求的成员自动成为领导者。你一定要注意区分这里的领导者和之前我们介绍的领导者副本，它们不是一个概念。这里的领导者是具体的消费者实例，它既不是副本，也不是协调者。领导者消费者的任务是收集所有成员的订阅信息，然后根据这些信息，制定具体的分区消费分配方案。\n选出领导者之后，协调者会把消费者组订阅信息封装进JoinGroup请求的响应体中，然后发给领导者，由领导者统一做出分配方案后，进入到下一步：发送SyncGroup请求。\n在这一步中，领导者向协调者发送SyncGroup请求，将刚刚做出的分配方案发给协调者。值得注意的是，其他成员也会向协调者发送SyncGroup请求，只不过请求体中并没有实际的内容。这一步的主要目的是让协调者接收分配方案，然后统一以SyncGroup响应的方式分发给所有成员，这样组内所有成员就都知道自己该消费哪些分区了。\nJoinGroup请求的主要作用是将组成员订阅信息发送给领导者消费者，待领导者制定好分配方案后，重平衡流程进入到SyncGroup请求阶段。\nSyncGroup请求的主要目的，就是让协调者把领导者制定的分配方案下发给各个组内成员。当所有成员都成功接收到分配方案后，消费者组进入到Stable状态，即开始正常的消费工作。\n正常情况下，每个组内成员都会定期汇报位移给协调者。当重平衡开启时，协调者会给予成员一段缓冲时间，要求每个成员必须在这段时间内快速地上报自己的位移信息，然后再开启正常的JoinGroup/SyncGroup请求发送。\nBroker Kafka集群包含一个或多个服务器，这种服务器被称为broker。一个 broker 可以容纳多个 topic。brocker 是 kafka 中的核心组件，负责消息的存储，分区，路由信息的管理等。","keywords":["Kafka"],"articleBody":"整体架构  图片来自 https://zhuanlan.zhihu.com/p/103249714\n Producer 负责发布消息到Kafka broker。Producer发送消息到broker时，会根据分区策略选择将其存储到哪一个Partition。\n常规的有轮询，随机等策略，主要是为了将消息均衡的发送到各个 partition，提高并行度，从而提高吞吐。常用的还有按 key 哈希，主要是为了实现业务 partition 有序的需求。\nConsumer 消息消费者，从Kafka broker读取消息的客户端。\nConsumer Group Consumer Group是Kafka提供的可扩展且具有容错性的消费者机制，每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。\n Consumer Group下可以有一个或多个Consumer实例。这里的实例可以是一个单独的进程，也可以是同一进程下的线程。 Group ID是一个字符串，在一个Kafka集群中，它标识唯一的一个Consumer Group。 Consumer Group下所有实例订阅的主题的单个分区，只能分配给组内的某个Consumer实例消费。  进度提交 消费者在消费的过程中需要记录自己消费了多少数据，即消费位置信息。在Kafka中，这个位置信息有个专门的术语：位移（Offset）。\nRebalance 何时触发 rebalance\n 组成员数量发生变化，有新成员加入，或者有成员实例崩溃退出。 订阅主题数量发生变化：Consumer Group可以使用正则表达式的方式订阅主题，比如果consumer.subscribe(Pattern.compile(“t.*c”))就表明该Group订阅所有以字母t开头、字母c结尾的主题。在Consumer Group的运行过程中，你新创建了一个满足这样条件的主题，那么该Group就会发生Rebalance 订阅主题的分区数发生变化，如主题扩容。  Rebalance本质上是一种协议，规定了一个Consumer Group下的所有Consumer如何达成一致，来分配订阅Topic的每个分区Topic的每个分区。\n比如某个Group下有20个Consumer实例，它订阅了一个具有100个分区的Topic。正常情况下，Kafka平均会为每个Consumer分配5个分区。这个分配的过程就叫Rebalance。\nRebalance 的流程 在消费者端，重平衡分为两个步骤：分别是 加入组 和 等待领导消费者（Leader Consumer）分配方案。这两个步骤分别对应两类特定的请求：JoinGroup请求和SyncGroup请求。\n当组内成员加入组时，它会向 协调者 发送JoinGroup请求（后面会介绍协调者）。在该请求中，每个成员都要将自己订阅的主题上报，这样协调者就能收集到所有成员的订阅信息。一旦收集了全部成员的JoinGroup请求后，协调者会从这些成员中选择一个担任这个消费者组的领导者。\n通常情况下，第一个发送JoinGroup请求的成员自动成为领导者。你一定要注意区分这里的领导者和之前我们介绍的领导者副本，它们不是一个概念。这里的领导者是具体的消费者实例，它既不是副本，也不是协调者。领导者消费者的任务是收集所有成员的订阅信息，然后根据这些信息，制定具体的分区消费分配方案。\n选出领导者之后，协调者会把消费者组订阅信息封装进JoinGroup请求的响应体中，然后发给领导者，由领导者统一做出分配方案后，进入到下一步：发送SyncGroup请求。\n在这一步中，领导者向协调者发送SyncGroup请求，将刚刚做出的分配方案发给协调者。值得注意的是，其他成员也会向协调者发送SyncGroup请求，只不过请求体中并没有实际的内容。这一步的主要目的是让协调者接收分配方案，然后统一以SyncGroup响应的方式分发给所有成员，这样组内所有成员就都知道自己该消费哪些分区了。\nJoinGroup请求的主要作用是将组成员订阅信息发送给领导者消费者，待领导者制定好分配方案后，重平衡流程进入到SyncGroup请求阶段。\nSyncGroup请求的主要目的，就是让协调者把领导者制定的分配方案下发给各个组内成员。当所有成员都成功接收到分配方案后，消费者组进入到Stable状态，即开始正常的消费工作。\n正常情况下，每个组内成员都会定期汇报位移给协调者。当重平衡开启时，协调者会给予成员一段缓冲时间，要求每个成员必须在这段时间内快速地上报自己的位移信息，然后再开启正常的JoinGroup/SyncGroup请求发送。\nBroker Kafka集群包含一个或多个服务器，这种服务器被称为broker。一个 broker 可以容纳多个 topic。brocker 是 kafka 中的核心组件，负责消息的存储，分区，路由信息的管理等。\nPartition Kafka中的分区机制指的是将每个主题划分成多个分区（Partition），每个分区是一组有序的消息日志。生产者生产的每条消息只会被发送到一个分区中，也就是说如果向一个双分区的主题发送一条消息，这条消息要么在分区0中，要么在分区1中。\n如你所见，Kafka的分区编号是从0开始的，如果Topic有100个分区，那么它们的分区号就是从0到99。讲到这里，你可能有这样的疑问：刚才提到的副本如何与这里的分区联系在一起呢？实际上，副本是在分区这个层级定义的。\n每个分区下可以配置若干个副本，其中只能有1个领导者副本和N-1个追随者副本。生产者向分区写入消息，每条消息在分区中的位置信息由一个叫位移（Offset）的数据来表征。分区位移总是从0开始，假设一个生产者向一个空分区写入了10条消息，那么这10条消息的位移依次是0、1、2、…、9。\nKafka Broker 使用消息日志（Log）来保存数据，一个日志就是磁盘上一个只能追加写（Append-only）消息的物理文件。因为只能追加写入，故避免了缓慢的随机I/O操作，改为性能较好的顺序I/O写操作，这也是实现Kafka高吞吐量特性的一个重要手段。\n不停地向一个日志写入消息，最终也会耗尽所有的磁盘空间，因此Kafka必然要定期地删除消息以回收磁盘。简单来说就是通过日志段（Log Segment）机制。在Kafka底层，一个日志又近一步细分成多个日志段，消息被追加写到当前最新的日志段中，当写满了一个日志段后，Kafka会自动切分出一个新的日志段，并将老的日志段封存起来。Kafka在后台还有定时任务会定期地检查老的日志段是否能够被删除，从而实现回收磁盘空间的目的。\nTopic Topic 是一种逻辑上的分区，是同一类消息的集合，每一个消息只能属于一个 Topic。\n为了使得Kafka的吞吐率提高，物理上把Topic分成一个或多个Partition，每个Partition在物理上对应一个文件夹，该文件夹下存储这个Partition的所有消息和索引文件。\n每个日志文件都是一个log entry序列，每个log entry包含一个4字节整型数值（值为N+5），1个字节的”magic value”，4个字节的CRC校验码，其后跟N个字节的消息体。每条消息都有一个当前Partition下唯一的64字节的offset，它指明了这条消息的起始位置。磁盘上存储的消息格式如下：\nmessage length ：4 bytes (value: 1+4+n) “magic” value ： 1 byte crc ： 4 bytes payload ： n bytes 这个log entry并非由一个文件构成，而是分成多个segment，每个segment以该segment第一条消息的offset命名并以“.kafka”为后缀。另外会有一个索引文件，它标明了每个segment下包含的log entry的offset范围。\n 图片来自：http://www.jasongj.com/2015/03/10/KafkaColumn1/\n 因为每条消息都被append到该Partition中，属于顺序写磁盘。但是如果 partition 过多，就会退化成随机写磁盘，因为 kafka 是每个 partition 一个目录去写，不同于 rokcetmq 的所有 queue 写一个文件。\nKafka 控制器 控制器组件（Controller），是Apache Kafka的核心组件。它的主要作用是在Apache ZooKeeper的帮助下管理和协调整个Kafka集群理和协调整个Kafka集群。集群中任意一台Broker都能充当控制器的角色，但是，在运行过程中，只能有一个Broker成为控制器，行使其管理和协调的职责。换句话说，每个正常运转的Kafka集群，在任意时刻都有且只有一个控制器\nBroker在启动时，会尝试去ZooKeeper中创建/controller节点。Kafka当前选举控制器的规则是：第一个成功创建/controller节点的Broker会被指定为控制器第一个成功创建/controller节点的Broker会被指定为控制器。控制器是做什么的？控制器是做什么的？我们经常说，控制器是起协调作用的组件，那么，这里的协调作用到底是指什么呢？我想了一下，控制器的职责大致可以分为5种，我们一起来看看。\n  主题管理（创建、删除、增加分区）：控制器帮助我们完成对Kafka主题的创建、删除以及分区增加的操作。\n  分区重分配。\n  Preferred领导者选举。\n  集群成员管理（新增Broker、Broker主动关闭、Broker宕机）：自动检测新增Broker、Broker主动关闭及被动宕机。\n这种自动检测是依赖于前面提到的Watch功能和ZooKeeper临时节点组合实现的。比如，控制器组件会利用Watch机制检 ZooKeeper的/brokers/ids节点下的子节点数量变更。目前，当有新Broker启动后，它会在/brokers下创建专属的znode节点。一旦创建完毕，ZooKeeper会通过Watch机制将消息通知推送给控制器，这样，控制器就能自动地感知到这个变化，进而开启后续的新增Broker作业。侦测Broker存活性则是依赖于刚刚提到的另一个机制：临时节点。每个Broker启动后，会在/brokers/ids下创建一个临时znode。当Broker宕机或主动关闭后，该Broker与ZooKeeper的会话结束，这个znode会被自动删除。同理，ZooKeeper的Watch机制将这一变更推送给控制器，这样控制器就能知道有Broker关闭或宕机了，从而进行“善后”。\n  向其他Broker提供数据服务：控制器上保存了最全的集群元数据信息，其他所有Broker会定期接收控制器发来的元数据更新请求，从而更新其内存中的缓存数据。\n这些数据其实在ZooKeeper中也保存了一份。每当控制器初始化时，它都会从ZooKeeper上读取对应的元数据并填充到自己的缓存中。有了这些数据，控制器就能对外提供数据服务了。这里的对外主要是指对其他Broker而言，控制器通过向这些Broker发送请求的方式将这些数据同步到其他Broker上。\n 所有主题信息。包括具体的分区信息，比如领导者副本是谁，ISR集合中有哪些副本等。 所有Broker信息。包括当前都有哪些运行中的Broker，哪些正在关闭中的Broker等。 所有涉及运维任务的分区。包括当前正在进行Preferred领导者选举以及分区重分配的分区列表。    控制器故障转移（Failover） 在Kafka集群运行过程中，只能有一台Broker充当控制器的角色，那么这就存在单点失效（Single Point of Failure）的风险。\n当运行中的控制器突然宕机或意外终止时，Kafka能够快速地感知到（通过 ZK 的 watch 机制），并立即重新选出新的的控制器。这个过程就被称为Failover，该过程是自动完成的，无需你手动干预。\n高水位和LeaderEpoch HW 和 LEO   LEO（last end offset）：日志末端位移，记录了该副本对象底层日志文件中下一条消息的位移值，副本写入消息的时候，会自动更新 LEO 值。\n  HW（high watermark）：HW 一定不会大于 LEO 值，小于 HW 值的消息被认为是“已提交，对消费者可见。\n  Log End Offset，简写是LEO，它表示副本写入下一条消息的位移值。同一个副本对象，其高水位值不会大于LEO值同一个副本对象。\nKafka所有副本都有对应的高水位和LEO值，而不仅仅是Leader副本。只不过Leader副本比较特殊，Kafka使用Leader副本的高水位来定义所在分区的高水位。\nleader 会保存两个类型的 LEO 值，一个是自己的 LEO，另一个是 remote LEO 值，remote LEO 值就是 follower 副本的 LEO 值，意味着 follower 副本的 LEO 值会保存两份，一份保存到 leader 副本中，一份保存到自己这里。\nLEO 更新机制：\n leader 副本自身的 LEO 值更新：在 Producer 消息发送过来时，即 leader 副本当前最新存储的消息位移位置 +1； follower 副本自身的 LEO 值更新：从 leader 副本中 fetch 到消息并写到本地日志文件时，即 follower 副本当前同步 leader 副本最新的消息位移位置 +1； leader 副本中的 remote LEO 值更新：每次 follower 副本发送 fetch 请求都会包含 follower 当前 LEO 值，leader 拿到该值就会尝试更新 remote LEO 值。  leader HW 更新机制：\nleader HW 更新分为故障时更新与正常时更新：\n故障时更新：\n 副本被选为 leader 副本时：当某个 follower 副本被选为分区的 leader 副本时，kafka 就会尝试更新 HW 值； 副本被踢出 ISR 时：如果某个副本追不上 leader 副本进度，或者所在 broker 崩溃了，导致被踢出 ISR，leader 也会检查 HW 值是否需要更新，毕竟 HW 值更新只跟处于 ISR 的副本 LEO 有关系。  正常时更新：\n producer 向 leader 副本写入消息时：在消息写入时会更新 leader LEO 值，因此需要再检查是否需要更新 HW 值； leader 处理 follower FETCH 请求时：follower 的 fetch 请求会携带 LEO 值，leader 会根据这个值更新对应的 remote LEO 值，同时也需要检查是否需要更新 HW 值。  follower HW 更新机制：\n follower 更新 HW 发生在其更新 LEO 之后，每次 follower Fetch 响应体都会包含 leader 的 HW 值，然后比较当前 LEO 值，取最小的作为新的 HW 值。  上图 stop4 说明 Follower副本的HW更新需要一轮额外的拉取请求才能实现。\n如果把上面那个例子扩展到多个Follower副本，情况可能更糟，也许需要多轮拉取请求。也就是说，Leader副本高水位更新和Follower副本高水位更新在时间上是存在错配的。这种错配是很多“数据丢失”或“数据不一致”问题的根源。\n数据丢失举例 第1步中的状态:\n 副本B作为leader收到producer的m2消息并写入本地文件，等待副本A拉取。 副本A发起消息拉取请求，请求中携带自己的最新的日志offset（LEO=1），B收到后更新自己的HW为1，并将HW=1的信息以及消息m2返回给A。 A收到拉取结果后更新本地的HW为1，并将m2写入本地文件。发起新一轮拉取请求（LEO=2），B收到A拉取请求后更新自己的HW为2，没有新数据只将HW=2的信息返回给A，并且回复给producer写入成功。此处的状态就是图中第一步的状态。  此时，如果没有异常，A会收到B的回复，得知目前的HW为2，然后更新自身的HW为2。\n但在第2步A重启了，没有来得及收到B的回复，此时B仍然是leader。\nA重启之后会以自己的 HW 为标准截断自己的日志，因为A作为follower不知道多出的日志是否是被提交过的，防止数据不一致从而截断多余的数据并尝试从leader那里重新同步\n第3步，B崩溃了，min.isr设置的是1，所以zookeeper会从ISRs中再选择一个作为leader，也就是A，但是A的数据不是完整的，从而出现了数据丢失现象。\nLeaderEpoch Leader Epoch，我们大致可以认为是Leader版本。它由两部分数据组成。\n  Epoch。一个单调增加的版本号。每当副本领导权发生变更时，都会增加该版本号。小版本号的Leader被认为是过期Leader，不能再行使Leader权力。\n  起始位移（Start Offset）。Leader副本在该Epoch值上写入的首条消息的位移。\n   举个例子来说明一下Leader Epoch。\n假设现在有两个Leader Epoch和，那么，第一个Leader Epoch表示版本号是0，这个版本的Leader从位移0开始保存消息，一共保存了120条消息。之后，Leader发生了变更，版本号增加到1，新版本的起始位移是120。\n Kafka Broker会在内存中为每个分区都缓存Leader Epoch数据，同时它还会定期地将这些信息持久化到一个checkpoint文件中。当Leader副本写入消息到磁盘时，Broker会尝试更新这部分缓存。如果该Leader是首次写入消息，那么Broker会向缓存中增加一个Leader Epoch条目，否则就不做更新。这样，每次有Leader变更时，新的Leader副本会查询这部分缓存，取出对应的Leader Epoch的起始位移，以避免数据丢失和不一致的情况\n引用Leader Epoch机制后，Follower副本B重启回来后，需要向A发送一个特殊的请求去获取Leader的LEO值。\n在这个例子中，该值为2。当获知到Leader LEO=2后，B发现该LEO值不比它自己的LEO值小，而且缓存中也没有保存任何起始位移值  2的Epoch条目，因此B无需执行任何日志截断操作。\nLeader Epoch 是对高水位机制的一个明显改进，即副本是否执行日志截断不再依赖于高水位进行判断。\n高可用 Replication 副本（Replica）本质是一个只能追加写消息的提交日志。根据Kafka副本机制的定义，同一个分区下的所有副本保存有相同的消息序列，这些副本分散保存在不同的Broker上。\nKafka定义了两类副本：领导者副本(Leader)和追随者副本(Follower)。一主多从，这些副本保存着相同的数据。领导者副本对外提供服务，追随者副本只是不断异步拉取领导者副本的消息，不对外提供服务。\n当领导者副本挂掉了，或者说领导者副本所在的Broker宕机时，Kafka依托于ZooKeeper提供的监控功能能够实时感知到，并立即开启新一轮的领导者选举，从追随者副本中选一个作为新的领导者。老Leader副本重启回来后，只能作为追随者副本加入到集群中。\n为什么kafka的追随者副本不对外服务  方便实现“Read-your-writes”。所谓Read-your-writes，顾名思义就是，当你使用生产者API向Kafka成功写入消息后，马上使用消费者API去读取刚才生产的消息。 方便实现单调读（Monotonic Reads）。什么是单调读呢？就是对于一个消费者用户而言，在多次消费消息时，它不会看到某条消息一会儿存在一会儿不存在。  ISR Kafka既不是完全的同步复制，也不是完全的异步复制，而是基于ISR的动态复制方案。\nISR 可以把他看成主从结构的升级版。对于每一个 partiton，可以分配一个或多个 Broker。 其中一个作为主节点，剩余的作为跟随者，跟随者会保存一个 partition 副本。生产者将消息发送到主节点后，主节点会广播给所有跟随者，跟随者收到后返回确认信息给主节点。 用户可以自由的配置副本数及当有几个副本写成功后，则认为消息成功保存。且同时，会在 ZooKeeper 上维护一个可用跟随者列表，列表中记录所有数据和主节点完全同步的跟随者列表。当主节点发生故障时，在列表中选择一个跟随者作为新的主节点提供服务。在这种策略下，假设总共有 m 个副本，要求至少有 n 个（0还有一种实现是基于 Raft 算法实现的多副本机制，具体细节可以参考官方的 paper。Raft 集群一般由奇数节点构成，如果要保证集群在 n 个节点故障的情况下可用，则至少需要有 2n+1 个节点。 与 ISR 方式相比，Raft 需要耗费更多的资源，但是整个复制和选举过程都是集群中的节点自主完成，不需要依赖 ZooKeeper 等第三者。 理论上 Raft 集群规模可以无限扩展而 ISR 模式下集群规模会受限于 ZooKeeper 集群的处理能力。\n ISR，也即In-sync Replica。每个Partition的Leader都会维护这样一个列表，该列表中，包含了所有与之同步的Replica（包含Leader自己）。每次数据写入时，只有ISR中的所有Replica都复制完，Leader才会将其置为Commit，它才能被Consumer所消费。\nBroker端参数 replica.lag.time.max.ms，这个参数的含义是Follower副本能够落后Leader副本的最长时间间隔，当前默认值是10秒。这就是说，只要一个Follower副本落后Leader副本的时间不连续超过10秒，那么Kafka就认为该Follower副本与Leader是同步的，即使此时Follower副本中保存的消息明显少于Leader副本中的消息。我们在前面说过，Follower副本唯一的工作就是不断地从Leader副本拉取消息，然后写入到自己的提交日志中。如果这个同步过程的速度持续慢于Leader副本的消息写入速度，那么在replica.lag.time.max.ms时间后，此Follower副本就会被认为是与Leader副本不同步的，因此不能再放入ISR中。此时，Kafka会自动收缩ISR集合，将该副本“踢出”ISR。值得注意的是，倘若该副本后面慢慢地追上了Leader的进度，那么它是能够重新被加回ISR的。这也表明，ISR是一个动态调整的集合，而非静态不变的。\nUnclean领导者选举（Unclean Leader Election） 既然ISR是可以动态调整的，那么自然就可以出现这样的情形：ISR为空。因为Leader副本天然就在ISR中，如果ISR为空了，就说明Leader副本也“挂掉”了，Kafka需要重新选举一个新的Leader。\n此时该怎么选举新Leader呢？Kafka把所有不在ISR中的存活副本都称为非同步副本Kafka把所有不在ISR中的存活副本都称为非同步副本。通常来说，非同步副本落后Leader太多，因此，如果选择这些副本作为新Leader，就可能出现数据的丢失。毕竟，这些副本中保存的消息远远落后于老Leader中的消息。在Kafka中，选举这种副本的过程称为Unclean领导者选举。Broker端参数Broker端参数unclean.leader.election.enable控制是否允许Unclean领导者选举unclean.leader.election.enable控制是否允许Unclean领导者选举。\n开启Unclean领导者选举可能会造成数据丢失，但好处是，它使得分区Leader副本一直存在，不至于停止对外提供服务，因此提升了高可用性。反之，禁止Unclean领导者选举的好处在于维护了数据的一致性，避免了消息丢失，但牺牲了高可用性。\n 推荐阅读\n为什么Kafka需要Leader Epoch\nKafka水位(high watermark)与leader epoch的讨论\nKafka高性能架构之道\n ","wordCount":"415","inLanguage":"en","datePublished":"2021-05-02T11:22:38+08:00","dateModified":"2021-05-02T11:22:38+08:00","author":{"@type":"Person","name":"徐旭"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mogutou.xyz/posts/mq/kafka/"},"publisher":{"@type":"Organization","name":"徐旭 的博客","logo":{"@type":"ImageObject","url":"https://mogutou.xyz/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(255, 255, 255, 0.84);--secondary:rgba(255, 255, 255, 0.56);--tertiary:rgba(255, 255, 255, 0.16);--content:rgba(255, 255, 255, 0.74);--hljs-bg:#2e2e33;--code-bg:#37383e;--border:#333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://mogutou.xyz/ accesskey=h title="徐旭 的博客 (Alt + H)">徐旭 的博客</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://mogutou.xyz/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://mogutou.xyz/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://mogutou.xyz/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://mogutou.xyz/about/ title="About Me"><span>About Me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mogutou.xyz/>Home</a>&nbsp;»&nbsp;<a href=https://mogutou.xyz/posts/>Posts</a></div><h1 class=post-title>Kafka 设计与理解</h1><div class=post-meta>May 2, 2021&nbsp;·&nbsp;2 min&nbsp;·&nbsp;徐旭</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=#%e6%95%b4%e4%bd%93%e6%9e%b6%e6%9e%84 aria-label=整体架构>整体架构</a></li><li><a href=#producer aria-label=Producer>Producer</a></li><li><a href=#consumer aria-label=Consumer>Consumer</a><ul><li><a href=#consumer-group aria-label="Consumer Group">Consumer Group</a></li><li><a href=#%e8%bf%9b%e5%ba%a6%e6%8f%90%e4%ba%a4 aria-label=进度提交>进度提交</a></li><li><a href=#rebalance aria-label=Rebalance>Rebalance</a><ul><li><a href=#rebalance-%e7%9a%84%e6%b5%81%e7%a8%8b aria-label="Rebalance 的流程">Rebalance 的流程</a></li></ul></li></ul></li><li><a href=#broker aria-label=Broker>Broker</a><ul><li><a href=#partition aria-label=Partition><strong>Partition</strong></a></li><li><a href=#topic aria-label=Topic>Topic</a></li><li><a href=#kafka-%e6%8e%a7%e5%88%b6%e5%99%a8 aria-label="Kafka 控制器">Kafka 控制器</a></li><li><a href=#%e6%8e%a7%e5%88%b6%e5%99%a8%e6%95%85%e9%9a%9c%e8%bd%ac%e7%a7%bbfailover aria-label=控制器故障转移（Failover）>控制器故障转移（Failover）</a></li><li><a href=#%e9%ab%98%e6%b0%b4%e4%bd%8d%e5%92%8cleaderepoch aria-label=高水位和LeaderEpoch>高水位和LeaderEpoch</a><ul><li><a href=#hw-%e5%92%8c-leo aria-label="HW 和 LEO">HW 和 LEO</a><ul><li><a href=#%e6%95%b0%e6%8d%ae%e4%b8%a2%e5%a4%b1%e4%b8%be%e4%be%8b aria-label=数据丢失举例>数据丢失举例</a></li></ul></li><li><a href=#leaderepoch aria-label=LeaderEpoch>LeaderEpoch</a></li></ul></li><li><a href=#%e9%ab%98%e5%8f%af%e7%94%a8 aria-label=高可用>高可用</a></li><li><a href=#replication aria-label=Replication>Replication</a><ul><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88kafka%e7%9a%84%e8%bf%bd%e9%9a%8f%e8%80%85%e5%89%af%e6%9c%ac%e4%b8%8d%e5%af%b9%e5%a4%96%e6%9c%8d%e5%8a%a1 aria-label=为什么kafka的追随者副本不对外服务>为什么kafka的追随者副本不对外服务</a></li><li><a href=#isr aria-label=ISR>ISR</a><ul><li><a href=#unclean%e9%a2%86%e5%af%bc%e8%80%85%e9%80%89%e4%b8%beunclean-leader-election aria-label="Unclean领导者选举（Unclean Leader Election）">Unclean领导者选举（Unclean Leader Election）</a></li></ul></li></ul></li></ul></li></ul></div></details></div><div class=post-content><h3 id=整体架构>整体架构<a hidden class=anchor aria-hidden=true href=#整体架构>#</a></h3><p><img loading=lazy src=https://cdn.jsdelivr.net/gh/Allenxuxu/blog/img/v2-aa36f2bbc1a6ff0d8f03aad80759bb01_r.jpg alt=preview></p><blockquote><p>图片来自 <a href=https://zhuanlan.zhihu.com/p/103249714>https://zhuanlan.zhihu.com/p/103249714</a></p></blockquote><h3 id=producer>Producer<a hidden class=anchor aria-hidden=true href=#producer>#</a></h3><p>负责发布消息到Kafka broker。Producer发送消息到broker时，会根据分区策略选择将其存储到哪一个Partition。</p><p>常规的有轮询，随机等策略，主要是为了将消息均衡的发送到各个 partition，提高并行度，从而提高吞吐。常用的还有按 key 哈希，主要是为了实现业务 partition 有序的需求。</p><h3 id=consumer>Consumer<a hidden class=anchor aria-hidden=true href=#consumer>#</a></h3><p>消息消费者，从Kafka broker读取消息的客户端。</p><h4 id=consumer-group>Consumer Group<a hidden class=anchor aria-hidden=true href=#consumer-group>#</a></h4><p>Consumer Group是Kafka提供的可扩展且具有容错性的消费者机制，每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。</p><ul><li>Consumer Group下可以有一个或多个Consumer实例。这里的实例可以是一个单独的进程，也可以是同一进程下的线程。</li><li>Group ID是一个字符串，在一个Kafka集群中，它标识唯一的一个Consumer Group。</li><li>Consumer Group下所有实例订阅的主题的单个分区，只能分配给组内的某个Consumer实例消费。</li></ul><h4 id=进度提交>进度提交<a hidden class=anchor aria-hidden=true href=#进度提交>#</a></h4><p>消费者在消费的过程中需要记录自己消费了多少数据，即消费位置信息。在Kafka中，这个位置信息有个专门的术语：位移（Offset）。</p><h4 id=rebalance>Rebalance<a hidden class=anchor aria-hidden=true href=#rebalance>#</a></h4><p>何时触发 rebalance</p><ol><li>组成员数量发生变化，有新成员加入，或者有成员实例崩溃退出。</li><li>订阅主题数量发生变化：Consumer Group可以使用正则表达式的方式订阅主题，比如果consumer.subscribe(Pattern.compile(“t.*c”))就表明该Group订阅所有以字母t开头、字母c结尾的主题。在Consumer Group的运行过程中，你新创建了一个满足这样条件的主题，那么该Group就会发生Rebalance</li><li>订阅主题的分区数发生变化，如主题扩容。</li></ol><p>Rebalance本质上是一种协议，规定了一个Consumer Group下的所有Consumer如何达成一致，来分配订阅Topic的每个分区Topic的每个分区。</p><p>比如某个Group下有20个Consumer实例，它订阅了一个具有100个分区的Topic。正常情况下，Kafka平均会为每个Consumer分配5个分区。这个分配的过程就叫Rebalance。</p><h5 id=rebalance-的流程>Rebalance 的流程<a hidden class=anchor aria-hidden=true href=#rebalance-的流程>#</a></h5><p>在消费者端，重平衡分为两个步骤：分别是 加入组 和 等待领导消费者（Leader Consumer）分配方案。这两个步骤分别对应两类特定的请求：JoinGroup请求和SyncGroup请求。</p><p>当组内成员加入组时，它会向 协调者 发送JoinGroup请求（后面会介绍协调者）。在该请求中，每个成员都要将自己订阅的主题上报，这样协调者就能收集到所有成员的订阅信息。一旦收集了全部成员的JoinGroup请求后，协调者会从这些成员中选择一个担任这个消费者组的领导者。</p><p>通常情况下，第一个发送JoinGroup请求的成员自动成为领导者。你一定要注意区分这里的领导者和之前我们介绍的领导者副本，它们不是一个概念。这里的领导者是具体的消费者实例，它既不是副本，也不是协调者。领导者消费者的任务是收集所有成员的订阅信息，然后根据这些信息，制定具体的分区消费分配方案。</p><p>选出领导者之后，协调者会把消费者组订阅信息封装进JoinGroup请求的响应体中，然后发给领导者，由领导者统一做出分配方案后，进入到下一步：发送SyncGroup请求。</p><p>在这一步中，领导者向协调者发送SyncGroup请求，将刚刚做出的分配方案发给协调者。值得注意的是，其他成员也会向协调者发送SyncGroup请求，只不过请求体中并没有实际的内容。这一步的主要目的是让协调者接收分配方案，然后统一以SyncGroup响应的方式分发给所有成员，这样组内所有成员就都知道自己该消费哪些分区了。</p><p><img loading=lazy src=https://cdn.jsdelivr.net/gh/Allenxuxu/blog/img/20210429091039.png alt=20210429091039></p><p>JoinGroup请求的主要作用是将组成员订阅信息发送给领导者消费者，待领导者制定好分配方案后，重平衡流程进入到SyncGroup请求阶段。</p><p><img loading=lazy src=https://cdn.jsdelivr.net/gh/Allenxuxu/blog/img/image-20210429091619717.png alt=image-20210429091619717></p><p>SyncGroup请求的主要目的，就是让协调者把领导者制定的分配方案下发给各个组内成员。当所有成员都成功接收到分配方案后，消费者组进入到Stable状态，即开始正常的消费工作。</p><p>正常情况下，每个组内成员都会定期汇报位移给协调者。当重平衡开启时，协调者会给予成员一段缓冲时间，要求每个成员必须在这段时间内快速地上报自己的位移信息，然后再开启正常的JoinGroup/SyncGroup请求发送。</p><h3 id=broker>Broker<a hidden class=anchor aria-hidden=true href=#broker>#</a></h3><p>Kafka集群包含一个或多个服务器，这种服务器被称为broker。一个 broker 可以容纳多个 topic。brocker 是 kafka 中的核心组件，负责消息的存储，分区，路由信息的管理等。</p><h4 id=partition><strong>Partition</strong><a hidden class=anchor aria-hidden=true href=#partition>#</a></h4><p>Kafka中的分区机制指的是将每个主题划分成多个分区（Partition），每个分区是一组有序的消息日志。生产者生产的每条消息只会被发送到一个分区中，也就是说如果向一个双分区的主题发送一条消息，这条消息要么在分区0中，要么在分区1中。</p><p>如你所见，Kafka的分区编号是从0开始的，如果Topic有100个分区，那么它们的分区号就是从0到99。讲到这里，你可能有这样的疑问：刚才提到的副本如何与这里的分区联系在一起呢？实际上，副本是在分区这个层级定义的。</p><p>每个分区下可以配置若干个副本，其中只能有1个领导者副本和N-1个追随者副本。生产者向分区写入消息，每条消息在分区中的位置信息由一个叫位移（Offset）的数据来表征。分区位移总是从0开始，假设一个生产者向一个空分区写入了10条消息，那么这10条消息的位移依次是0、1、2、&mldr;、9。</p><p>Kafka Broker 使用消息日志（Log）来保存数据，一个日志就是磁盘上一个只能追加写（Append-only）消息的物理文件。因为只能追加写入，故避免了缓慢的随机I/O操作，改为性能较好的顺序I/O写操作，这也是实现Kafka高吞吐量特性的一个重要手段。</p><p>不停地向一个日志写入消息，最终也会耗尽所有的磁盘空间，因此Kafka必然要定期地删除消息以回收磁盘。简单来说就是通过日志段（Log Segment）机制。在Kafka底层，一个日志又近一步细分成多个日志段，消息被追加写到当前最新的日志段中，当写满了一个日志段后，Kafka会自动切分出一个新的日志段，并将老的日志段封存起来。Kafka在后台还有定时任务会定期地检查老的日志段是否能够被删除，从而实现回收磁盘空间的目的。</p><h4 id=topic>Topic<a hidden class=anchor aria-hidden=true href=#topic>#</a></h4><p>Topic 是一种逻辑上的分区，是同一类消息的集合，每一个消息只能属于一个 Topic。</p><p>为了使得Kafka的吞吐率提高，物理上把Topic分成一个或多个Partition，每个Partition在物理上对应一个文件夹，该文件夹下存储这个Partition的所有消息和索引文件。</p><p>每个日志文件都是一个<code>log entry</code>序列，每个<code>log entry</code>包含一个4字节整型数值（值为N+5），1个字节的”magic value”，4个字节的CRC校验码，其后跟N个字节的消息体。每条消息都有一个当前Partition下唯一的64字节的offset，它指明了这条消息的起始位置。磁盘上存储的消息格式如下：</p><pre><code class=language-　　 data-lang=　　>message length ：4 bytes (value: 1+4+n)
“magic” value ： 1 byte
crc ： 					4 bytes
payload ： 			n bytes
</code></pre><p>这个<code>log entry</code>并非由一个文件构成，而是分成多个segment，每个segment以该segment第一条消息的offset命名并以“.kafka”为后缀。另外会有一个索引文件，它标明了每个segment下包含的<code>log entry</code>的offset范围。</p><p><img loading=lazy src=https://cdn.jsdelivr.net/gh/Allenxuxu/blog/img/partition_segment.png alt=img></p><blockquote><p>图片来自：http://www.jasongj.com/2015/03/10/KafkaColumn1/</p></blockquote><p>因为每条消息都被append到该Partition中，属于顺序写磁盘。但是如果 partition 过多，就会退化成随机写磁盘，因为 kafka 是每个 partition 一个目录去写，不同于 rokcetmq 的所有 queue 写一个文件。</p><h4 id=kafka-控制器>Kafka 控制器<a hidden class=anchor aria-hidden=true href=#kafka-控制器>#</a></h4><p>控制器组件（Controller），是Apache Kafka的核心组件。它的主要作用是在Apache ZooKeeper的帮助下管理和协调整个Kafka集群理和协调整个Kafka集群。集群中任意一台Broker都能充当控制器的角色，但是，在运行过程中，只能有一个Broker成为控制器，行使其管理和协调的职责。换句话说，每个正常运转的Kafka集群，在任意时刻都有且只有一个控制器</p><p>Broker在启动时，会尝试去ZooKeeper中创建/controller节点。Kafka当前选举控制器的规则是：第一个成功创建/controller节点的Broker会被指定为控制器第一个成功创建/controller节点的Broker会被指定为控制器。控制器是做什么的？控制器是做什么的？我们经常说，控制器是起协调作用的组件，那么，这里的协调作用到底是指什么呢？我想了一下，控制器的职责大致可以分为5种，我们一起来看看。</p><ul><li><p>主题管理（创建、删除、增加分区）：控制器帮助我们完成对Kafka主题的创建、删除以及分区增加的操作。</p></li><li><p>分区重分配。</p></li><li><p>Preferred领导者选举。</p></li><li><p>集群成员管理（新增Broker、Broker主动关闭、Broker宕机）：自动检测新增Broker、Broker主动关闭及被动宕机。</p><p>这种自动检测是依赖于前面提到的Watch功能和ZooKeeper临时节点组合实现的。比如，控制器组件会利用Watch机制检 ZooKeeper的<code>/brokers/ids</code>节点下的子节点数量变更。目前，当有新Broker启动后，它会在<code>/brokers</code>下创建专属的znode节点。一旦创建完毕，ZooKeeper会通过Watch机制将消息通知推送给控制器，这样，控制器就能自动地感知到这个变化，进而开启后续的新增Broker作业。侦测Broker存活性则是依赖于刚刚提到的另一个机制：临时节点。每个Broker启动后，会在<code>/brokers/ids</code>下创建一个临时znode。当Broker宕机或主动关闭后，该Broker与ZooKeeper的会话结束，这个znode会被自动删除。同理，ZooKeeper的Watch机制将这一变更推送给控制器，这样控制器就能知道有Broker关闭或宕机了，从而进行“善后”。</p></li><li><p>向其他Broker提供数据服务：控制器上保存了最全的集群元数据信息，其他所有Broker会定期接收控制器发来的元数据更新请求，从而更新其内存中的缓存数据。</p><p>这些数据其实在ZooKeeper中也保存了一份。每当控制器初始化时，它都会从ZooKeeper上读取对应的元数据并填充到自己的缓存中。有了这些数据，控制器就能对外提供数据服务了。这里的对外主要是指对其他Broker而言，控制器通过向这些Broker发送请求的方式将这些数据同步到其他Broker上。</p><ul><li>所有主题信息。包括具体的分区信息，比如领导者副本是谁，ISR集合中有哪些副本等。</li><li>所有Broker信息。包括当前都有哪些运行中的Broker，哪些正在关闭中的Broker等。</li><li>所有涉及运维任务的分区。包括当前正在进行Preferred领导者选举以及分区重分配的分区列表。</li></ul></li></ul><h4 id=控制器故障转移failover>控制器故障转移（Failover）<a hidden class=anchor aria-hidden=true href=#控制器故障转移failover>#</a></h4><p>在Kafka集群运行过程中，只能有一台Broker充当控制器的角色，那么这就存在单点失效（Single Point of Failure）的风险。</p><p>当运行中的控制器突然宕机或意外终止时，Kafka能够快速地感知到（通过 ZK 的 watch 机制），并立即重新选出新的的控制器。这个过程就被称为Failover，该过程是自动完成的，无需你手动干预。</p><h4 id=高水位和leaderepoch>高水位和LeaderEpoch<a hidden class=anchor aria-hidden=true href=#高水位和leaderepoch>#</a></h4><h5 id=hw-和-leo>HW 和 LEO<a hidden class=anchor aria-hidden=true href=#hw-和-leo>#</a></h5><ul><li><p>LEO（last end offset）：日志末端位移，记录了该副本对象底层日志文件中下一条消息的位移值，副本写入消息的时候，会自动更新 LEO 值。</p></li><li><p>HW（high watermark）：HW 一定不会大于 LEO 值，小于 HW 值的消息被认为是“已提交，对消费者可见。</p></li></ul><p><img loading=lazy src=https://cdn.jsdelivr.net/gh/Allenxuxu/blog/img/image-20210502100333745.png alt=image-20210502100333745></p><p>Log End Offset，简写是LEO，它表示副本写入下一条消息的位移值。同一个副本对象，其高水位值不会大于LEO值同一个副本对象。</p><p>Kafka所有副本都有对应的高水位和LEO值，而不仅仅是Leader副本。只不过Leader副本比较特殊，Kafka使用Leader副本的高水位来定义所在分区的高水位。</p><p>leader 会保存两个类型的 LEO 值，一个是自己的 LEO，另一个是 remote LEO 值，remote LEO 值就是 follower 副本的 LEO 值，意味着 follower 副本的 LEO 值会保存两份，一份保存到 leader 副本中，一份保存到自己这里。</p><p><strong>LEO 更新机制：</strong></p><ol><li>leader 副本自身的 LEO 值更新：在 Producer 消息发送过来时，即 leader 副本当前最新存储的消息位移位置 +1；</li><li>follower 副本自身的 LEO 值更新：从 leader 副本中 fetch 到消息并写到本地日志文件时，即 follower 副本当前同步 leader 副本最新的消息位移位置 +1；</li><li>leader 副本中的 remote LEO 值更新：每次 follower 副本发送 fetch 请求都会包含 follower 当前 LEO 值，leader 拿到该值就会尝试更新 remote LEO 值。</li></ol><p><strong>leader HW 更新机制：</strong></p><p>leader HW 更新分为故障时更新与正常时更新：</p><p>故障时更新：</p><ol><li>副本被选为 leader 副本时：当某个 follower 副本被选为分区的 leader 副本时，kafka 就会尝试更新 HW 值；</li><li>副本被踢出 ISR 时：如果某个副本追不上 leader 副本进度，或者所在 broker 崩溃了，导致被踢出 ISR，leader 也会检查 HW 值是否需要更新，毕竟 HW 值更新只跟处于 ISR 的副本 LEO 有关系。</li></ol><p>正常时更新：</p><ol><li>producer 向 leader 副本写入消息时：在消息写入时会更新 leader LEO 值，因此需要再检查是否需要更新 HW 值；</li><li>leader 处理 follower FETCH 请求时：follower 的 fetch 请求会携带 LEO 值，leader 会根据这个值更新对应的 remote LEO 值，同时也需要检查是否需要更新 HW 值。</li></ol><p><strong>follower HW 更新机制：</strong></p><ol><li>follower 更新 HW 发生在其更新 LEO 之后，每次 follower Fetch 响应体都会包含 leader 的 HW 值，然后比较当前 LEO 值，取最小的作为新的 HW 值。</li></ol><p><img loading=lazy src=https://cdn.jsdelivr.net/gh/Allenxuxu/blog/img/20191030190621.png alt=img></p><p>上图 stop4 说明 Follower副本的HW更新需要一轮额外的拉取请求才能实现。</p><p>如果把上面那个例子扩展到多个Follower副本，情况可能更糟，也许需要多轮拉取请求。也就是说，Leader副本高水位更新和Follower副本高水位更新在时间上是存在错配的。这种错配是很多“数据丢失”或“数据不一致”问题的根源。</p><h6 id=数据丢失举例>数据丢失举例<a hidden class=anchor aria-hidden=true href=#数据丢失举例>#</a></h6><p><img loading=lazy src=https://cdn.jsdelivr.net/gh/Allenxuxu/blog/img/kafka_leader_epoch_error_1.jpg alt=img></p><p>第1步中的状态:</p><ol><li>副本B作为leader收到producer的m2消息并写入本地文件，等待副本A拉取。</li><li>副本A发起消息拉取请求，请求中携带自己的最新的日志offset（LEO=1），B收到后更新自己的HW为1，并将HW=1的信息以及消息m2返回给A。</li><li>A收到拉取结果后更新本地的HW为1，并将m2写入本地文件。发起新一轮拉取请求（LEO=2），B收到A拉取请求后更新自己的HW为2，没有新数据只将HW=2的信息返回给A，并且回复给producer写入成功。此处的状态就是图中第一步的状态。</li></ol><p>此时，如果没有异常，A会收到B的回复，得知目前的HW为2，然后更新自身的HW为2。</p><p>但在第2步A重启了，没有来得及收到B的回复，此时B仍然是leader。</p><p><strong>A重启之后会以自己的 HW 为标准截断自己的日志，因为A作为follower不知道多出的日志是否是被提交过的，防止数据不一致从而截断多余的数据并尝试从leader那里重新同步</strong></p><p>第3步，B崩溃了，min.isr设置的是1，所以zookeeper会从ISRs中再选择一个作为leader，也就是A，但是A的数据不是完整的，从而出现了数据丢失现象。</p><h5 id=leaderepoch>LeaderEpoch<a hidden class=anchor aria-hidden=true href=#leaderepoch>#</a></h5><p>Leader Epoch，我们大致可以认为是Leader版本。它由两部分数据组成。</p><ul><li><p>Epoch。一个单调增加的版本号。每当副本领导权发生变更时，都会增加该版本号。小版本号的Leader被认为是过期Leader，不能再行使Leader权力。</p></li><li><p>起始位移（Start Offset）。Leader副本在该Epoch值上写入的首条消息的位移。</p></li></ul><blockquote><p>举个例子来说明一下Leader Epoch。</p><p>假设现在有两个Leader Epoch&lt;0, 0>和&lt;1, 120>，那么，第一个Leader Epoch表示版本号是0，这个版本的Leader从位移0开始保存消息，一共保存了120条消息。之后，Leader发生了变更，版本号增加到1，新版本的起始位移是120。</p></blockquote><p>Kafka Broker会在内存中为每个分区都缓存Leader Epoch数据，同时它还会定期地将这些信息持久化到一个checkpoint文件中。当Leader副本写入消息到磁盘时，Broker会尝试更新这部分缓存。如果该Leader是首次写入消息，那么Broker会向缓存中增加一个Leader Epoch条目，否则就不做更新。这样，每次有Leader变更时，新的Leader副本会查询这部分缓存，取出对应的Leader Epoch的起始位移，以避免数据丢失和不一致的情况</p><p><img loading=lazy src=https://cdn.jsdelivr.net/gh/Allenxuxu/blog/img/kafka_leader_epoch_solve_1.jpg alt=img></p><p>引用Leader Epoch机制后，Follower副本B重启回来后，需要向A发送一个特殊的请求去获取Leader的LEO值。</p><p>在这个例子中，该值为2。当获知到Leader LEO=2后，B发现该LEO值不比它自己的LEO值小，而且缓存中也没有保存任何起始位移值 > 2的Epoch条目，因此B无需执行任何日志截断操作。</p><p>Leader Epoch 是对高水位机制的一个明显改进，即副本是否执行日志截断不再依赖于高水位进行判断。</p><h4 id=高可用>高可用<a hidden class=anchor aria-hidden=true href=#高可用>#</a></h4><h4 id=replication>Replication<a hidden class=anchor aria-hidden=true href=#replication>#</a></h4><p>副本（Replica）本质是一个只能追加写消息的提交日志。根据Kafka副本机制的定义，同一个分区下的所有副本保存有相同的消息序列，这些副本分散保存在不同的Broker上。</p><p>Kafka定义了两类副本：领导者副本(Leader)和追随者副本(Follower)。一主多从，这些副本保存着相同的数据。领导者副本对外提供服务，追随者副本只是不断异步拉取领导者副本的消息，不对外提供服务。</p><p>当领导者副本挂掉了，或者说领导者副本所在的Broker宕机时，Kafka依托于ZooKeeper提供的监控功能能够实时感知到，并立即开启新一轮的领导者选举，从追随者副本中选一个作为新的领导者。老Leader副本重启回来后，只能作为追随者副本加入到集群中。</p><h5 id=为什么kafka的追随者副本不对外服务>为什么kafka的追随者副本不对外服务<a hidden class=anchor aria-hidden=true href=#为什么kafka的追随者副本不对外服务>#</a></h5><ul><li>方便实现“Read-your-writes”。所谓Read-your-writes，顾名思义就是，当你使用生产者API向Kafka成功写入消息后，马上使用消费者API去读取刚才生产的消息。</li><li>方便实现单调读（Monotonic Reads）。什么是单调读呢？就是对于一个消费者用户而言，在多次消费消息时，它不会看到某条消息一会儿存在一会儿不存在。</li></ul><h5 id=isr>ISR<a hidden class=anchor aria-hidden=true href=#isr>#</a></h5><p>Kafka既不是完全的同步复制，也不是完全的异步复制，而是基于ISR的动态复制方案。</p><p>ISR 可以把他看成主从结构的升级版。对于每一个 partiton，可以分配一个或多个 Broker。 其中一个作为主节点，剩余的作为跟随者，跟随者会保存一个 partition 副本。生产者将消息发送到主节点后，主节点会广播给所有跟随者，跟随者收到后返回确认信息给主节点。 用户可以自由的配置副本数及当有几个副本写成功后，则认为消息成功保存。且同时，会在 ZooKeeper 上维护一个可用跟随者列表，列表中记录所有数据和主节点完全同步的跟随者列表。当主节点发生故障时，在列表中选择一个跟随者作为新的主节点提供服务。在这种策略下，假设总共有 m 个副本，要求至少有 n 个（0&lt;n&lt;m+1）副本写成功，则系统可以在最多 m-n 个机器故障的情况下保证可用性。</p><blockquote><p>还有一种实现是基于 Raft 算法实现的多副本机制，具体细节可以参考官方的 paper。Raft 集群一般由奇数节点构成，如果要保证集群在 n 个节点故障的情况下可用，则至少需要有 2n+1 个节点。 与 ISR 方式相比，Raft 需要耗费更多的资源，但是整个复制和选举过程都是集群中的节点自主完成，不需要依赖 ZooKeeper 等第三者。 理论上 Raft 集群规模可以无限扩展而 ISR 模式下集群规模会受限于 ZooKeeper 集群的处理能力。</p></blockquote><p>ISR，也即In-sync Replica。每个Partition的Leader都会维护这样一个列表，该列表中，包含了所有与之同步的Replica（包含Leader自己）。每次数据写入时，只有ISR中的所有Replica都复制完，Leader才会将其置为Commit，它才能被Consumer所消费。</p><p>Broker端参数 <code>replica.lag.time.max.ms</code>，这个参数的含义是Follower副本能够落后Leader副本的最长时间间隔，当前默认值是10秒。这就是说，只要一个Follower副本落后Leader副本的时间不连续超过10秒，那么Kafka就认为该Follower副本与Leader是同步的，即使此时Follower副本中保存的消息明显少于Leader副本中的消息。我们在前面说过，Follower副本唯一的工作就是不断地从Leader副本拉取消息，然后写入到自己的提交日志中。如果这个同步过程的速度持续慢于Leader副本的消息写入速度，那么在replica.lag.time.max.ms时间后，此Follower副本就会被认为是与Leader副本不同步的，因此不能再放入ISR中。此时，Kafka会自动收缩ISR集合，将该副本“踢出”ISR。值得注意的是，倘若该副本后面慢慢地追上了Leader的进度，那么它是能够重新被加回ISR的。这也表明，ISR是一个动态调整的集合，而非静态不变的。</p><h6 id=unclean领导者选举unclean-leader-election>Unclean领导者选举（Unclean Leader Election）<a hidden class=anchor aria-hidden=true href=#unclean领导者选举unclean-leader-election>#</a></h6><p>既然ISR是可以动态调整的，那么自然就可以出现这样的情形：ISR为空。因为Leader副本天然就在ISR中，如果ISR为空了，就说明Leader副本也“挂掉”了，Kafka需要重新选举一个新的Leader。</p><p>此时该怎么选举新Leader呢？Kafka把所有不在ISR中的存活副本都称为非同步副本Kafka把所有不在ISR中的存活副本都称为非同步副本。通常来说，非同步副本落后Leader太多，因此，如果选择这些副本作为新Leader，就可能出现数据的丢失。毕竟，这些副本中保存的消息远远落后于老Leader中的消息。在Kafka中，选举这种副本的过程称为Unclean领导者选举。Broker端参数Broker端参数unclean.leader.election.enable控制是否允许Unclean领导者选举unclean.leader.election.enable控制是否允许Unclean领导者选举。</p><p>开启Unclean领导者选举可能会造成数据丢失，但好处是，它使得分区Leader副本一直存在，不至于停止对外提供服务，因此提升了高可用性。反之，禁止Unclean领导者选举的好处在于维护了数据的一致性，避免了消息丢失，但牺牲了高可用性。</p><blockquote><p>推荐阅读</p><p><a href=https://t1mek1ller.github.io/2020/02/15/kafka-leader-epoch/>为什么Kafka需要Leader Epoch</a></p><p><a href=https://www.cnblogs.com/huxi2b/p/7453543.html>Kafka水位(high watermark)与leader epoch的讨论</a></p><p><a href=http://www.jasongj.com/kafka/high_throughput/>Kafka高性能架构之道</a></p></blockquote></div><footer class=post-footer><ul class=post-tags><li><a href=https://mogutou.xyz/tags/kafka/>Kafka</a></li></ul><nav class=paginav><a class=next href=https://mogutou.xyz/posts/practice/gin-vue/><span class=title>Next Page »</span><br><span>Go1.16 embed 和 Vue</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Kafka 设计与理解 on twitter" href="https://twitter.com/intent/tweet/?text=Kafka%20%e8%ae%be%e8%ae%a1%e4%b8%8e%e7%90%86%e8%a7%a3&url=https%3a%2f%2fmogutou.xyz%2fposts%2fmq%2fkafka%2f&hashtags=Kafka"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Kafka 设计与理解 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fmogutou.xyz%2fposts%2fmq%2fkafka%2f&title=Kafka%20%e8%ae%be%e8%ae%a1%e4%b8%8e%e7%90%86%e8%a7%a3&summary=Kafka%20%e8%ae%be%e8%ae%a1%e4%b8%8e%e7%90%86%e8%a7%a3&source=https%3a%2f%2fmogutou.xyz%2fposts%2fmq%2fkafka%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Kafka 设计与理解 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmogutou.xyz%2fposts%2fmq%2fkafka%2f&title=Kafka%20%e8%ae%be%e8%ae%a1%e4%b8%8e%e7%90%86%e8%a7%a3"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Kafka 设计与理解 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmogutou.xyz%2fposts%2fmq%2fkafka%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Kafka 设计与理解 on whatsapp" href="https://api.whatsapp.com/send?text=Kafka%20%e8%ae%be%e8%ae%a1%e4%b8%8e%e7%90%86%e8%a7%a3%20-%20https%3a%2f%2fmogutou.xyz%2fposts%2fmq%2fkafka%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Kafka 设计与理解 on telegram" href="https://telegram.me/share/url?text=Kafka%20%e8%ae%be%e8%ae%a1%e4%b8%8e%e7%90%86%e8%a7%a3&url=https%3a%2f%2fmogutou.xyz%2fposts%2fmq%2fkafka%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=Allenxuxu/Allenxuxu.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2021 <a href=https://mogutou.xyz/>徐旭 的博客</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position"))};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft)}document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script><script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script></body></html>